syntax = "proto3";

package train_2048.inference.v1;

// High-level contract for 2048 inference. Versioned under `v1` for evolvability.
// This variant returns per-head probability distributions over bins and avoids
// leaking backend/dtype/padding details into the wire format.

// A single inference item: caller-provided id and a compact board encoding.
message Item {
  // Stable, caller-supplied identifier for routing results.
  uint64 id = 1;
  // Board exponents in row-major order. Exactly 16 bytes for a 4x4 board.
  // 0 = empty, 1 = 2, 2 = 4, ...
  bytes board = 2; // len == 16
}

// Request a batched inference over one or more boards.
message InferRequest {
  // Optional logical identifier of the model or init to route against.
  string model_id = 1;

  // Batch of inputs to evaluate.
  repeated Item items = 2;

  // Optional batch identifier for telemetry/correlation.
  uint64 batch_id = 3;

  // If true, include a pooled embedding for each item in the response.
  // Embedding is returned as raw bytes; see InferResponse.embed_dtype.
  bool return_embedding = 4;

  // If true, the server MAY skip returning full per-head distributions and
  // instead emit argmax-only decisions (see InferResponse.argmax_heads /
  // argmax_p1). Clients should fall back to full distributions when unset.
  bool argmax_only = 5;
}

// A single head's distribution as probabilities over bins.
message HeadProbs {
  // Probabilities (sum ~ 1.0). len == n_bins, shared across heads.
  repeated float probs = 1;
}

// Per-board output: per-head probability distributions.
// Heads are ordered [Up, Down, Left, Right] to match dataset order.
message Output {
  // Shape conceptually: (4, n_bins)
  repeated HeadProbs heads = 1;

  // Optional pooled embedding for this item, encoded per InferResponse.embed_dtype.
  // When empty, no embedding was requested or returned.
  bytes embedding = 2;
}

// Metadata describing model/tokenizer semantics for policy heads.
message PolicyMetadata {
  // Logical head type (e.g., "binned_ev", "action_policy").
  string head_type = 1;
  // Number of bins emitted per head (shared across heads).
  uint32 bin_count = 2;
  // Optional tokenizer identifier (e.g., "macroxue_ev_advantage_v2").
  string tokenizer_type = 3;
  // Optional human-readable labels for each bin (ILLEGAL, FAILURE, ... WINNER).
  repeated string vocab_labels = 4;
}

message ModelMetadata {
  // Policy/tokenizer metadata; future fields may cover embeddings/value heads.
  PolicyMetadata policy = 1;
}

// Batched inference response.
message InferResponse {
  // Echoed from request if provided.
  uint64 batch_id = 1;

  // Item identifiers in the same order as outputs.
  repeated uint64 item_ids = 2;

  // One output per input board (same order as item_ids).
  repeated Output outputs = 3;

  // Server-measured latency in milliseconds for the batch, if available.
  uint64 latency_ms = 4;

  // Embedding metadata (present when return_embedding was true).
  // All embeddings in this response share the same shape and dtype.
  uint32 embed_dim = 5;
  enum EmbedDType {
    EMBED_DTYPE_UNSPECIFIED = 0;
    FP32 = 1;  // float32 bytes, little-endian
    BF16 = 2;  // bfloat16 16-bit words, little-endian
  }
  EmbedDType embed_dtype = 6;

  // Optional: when embeddings are requested, servers MAY return a single
  // concatenated buffer containing all embeddings in batch order to avoid
  // per-item serialization overhead. The buffer layout is:
  //   [item0 (embed_dim * sizeof(dtype))][item1][...][itemB-1]
  // Clients should prefer this field when present; fall back to per-item
  // Output.embedding otherwise. The number of items is `len(item_ids)`.
  bytes embeddings_concat = 7;

  // When InferRequest.argmax_only=true, servers MAY populate these fields
  // with the argmax head index (0=Up,1=Down,2=Left,3=Right) and its
  // corresponding probability mass in the final bin (p1). Length matches
  // len(item_ids). When empty, fall back to `outputs`.
  repeated uint32 argmax_heads = 8;
  repeated float argmax_p1 = 9;

  // Optional metadata describing model/tokenizer semantics.
  ModelMetadata model_metadata = 10;
}

service Inference {
  // Compute per-head probability distributions over bins for each input.
  rpc Infer (InferRequest) returns (InferResponse);
}
