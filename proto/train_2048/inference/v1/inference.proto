syntax = "proto3";

package train_2048.inference.v1;
// Avoid collision with the Python package named `train_2048` by directing
// Python codegen to place modules under a different package path.
option python_package = "infer_2048.proto.train_2048.inference.v1";

// High-level contract for 2048 inference. Versioned under `v1` for evolvability.
// This variant returns per-head probability distributions over bins and avoids
// leaking backend/dtype/padding details into the wire format.

// A single inference item: caller-provided id and a compact board encoding.
message Item {
  // Stable, caller-supplied identifier for routing results.
  uint64 id = 1;
  // Board exponents in row-major order. Exactly 16 bytes for a 4x4 board.
  // 0 = empty, 1 = 2, 2 = 4, ...
  bytes board = 2; // len == 16
}

// Request a batched inference over one or more boards.
message InferRequest {
  // Optional logical identifier of the model or init to route against.
  string model_id = 1;

  // Batch of inputs to evaluate.
  repeated Item items = 2;

  // Optional batch identifier for telemetry/correlation.
  uint64 batch_id = 3;
}

// A single head's distribution as probabilities over bins.
message HeadProbs {
  // Probabilities (sum ~ 1.0). len == n_bins, shared across heads.
  repeated float probs = 1;
}

// Per-board output: per-head probability distributions.
// Heads are ordered [Up, Down, Left, Right] to match dataset order.
message Output {
  // Shape conceptually: (4, n_bins)
  repeated HeadProbs heads = 1;
}

// Batched inference response.
message InferResponse {
  // Echoed from request if provided.
  uint64 batch_id = 1;

  // Item identifiers in the same order as outputs.
  repeated uint64 item_ids = 2;

  // One output per input board (same order as item_ids).
  repeated Output outputs = 3;

  // Server-measured latency in milliseconds for the batch, if available.
  uint64 latency_ms = 4;
}

service Inference {
  // Compute per-head probability distributions over bins for each input.
  rpc Infer (InferRequest) returns (InferResponse);
}
