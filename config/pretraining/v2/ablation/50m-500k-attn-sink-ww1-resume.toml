# Ablation: attention sink + winner_weight=1.0 (baseline)
# 500k continuation run from checkpoint init

init_dir = "checkpoints/20260110_014509/model-stable.pt"
checkpoint_dir = "checkpoints"
seed = 42

[wandb]
enabled = true
project = "train-2048-bin-v2"
entity = ""
# run_name = "50m-500k-attn-sink-ww1-resume"
tags = ["v2", "macroxue-binned", "attn-sink", "ablation", "autocast", "resume-500k"]
mode = "online"
report_every = 50

[target]
mode = "macroxue_tokens"
winner_weight = 1.0

[hyperparameters]
learning_rate = 8e-4
grad_clip_norm = 1.0

[hyperparameters.lr_schedule]
name = "linear-decay-then-cosine"
linear_start_step = 225_000
linear_steps = 275_000
intermediate_ratio = 0.5
min_lr_ratio = 0.05

[hyperparameters.optimizer]
name = "adamw"
weight_decay = 0.01
beta1 = 0.9
beta2 = 0.98
eps = 1e-8

[batch]
batch_size = 2048

[dropout]
dropout_prob = 0.03
attention_dropout_prob = 0.0

[dataset]
dataset_dir = "datasets/d6_3b_v0"
tokenizer_path = "out/macroxue_v2/tokenizer.json"
mmap_mode = true
num_steps = 1_000_000
shard_locality = true
shard_cache_in_memory = true
shard_cache_keep_shards = 1

val_run_pct = 0.005
val_num_steps = 200
val_split_seed = 42
val_every = 5_000

[dataset.rotation_augment]
mode = "random_k"
allow_noop = true

[dataset.flip_augment]
mode = "random_axis"
allow_noop = true

[checkpoint]
every_epochs = 1
save_best_every_steps = 10000
best_min_delta = 0.002
save_pt_every_steps = 250_000

[grad_logging]
norm_every_steps = 1000
dump_every_steps = 50_000
dump_param_names = [
  "tok_emb.weight",
  "blocks.0.attn.wq.weight",
  "blocks.0.attn.wk.weight",
  "blocks.0.attn.wv.weight",
  "blocks.0.mlp.w_up.weight",
  "blocks.12.attn.wq.weight",
  "blocks.12.mlp.w_up.weight",
  "blocks.23.attn.wq.weight",
  "blocks.23.mlp.w_up.weight",
  "ev_heads.0.weight",
]
