init_dir = "inits/v1_10m"      # Directory with config & weights or a checkpoint .pt file
checkpoint_dir = "checkpoints"
seed = 0

[wandb]
enabled = true
project = "train-2048"
entity = ""
run_name = ""
tags = []
mode = "online"        # online | offline | disabled
report_every = 10       # log to W&B every N steps (>=1)

[hyperparameters]
learning_rate = 4e-4
muon_lr = 2e-2
# Clip global gradient norm each step (None to disable)
grad_clip_norm = 1.0

[hyperparameters.lr_schedule]
name = "warmup-stable-decay" # constant | warmup-stable-decay | cosine
warmup_steps = 1000
# decay_steps = 0
# Alternatively, specify a percentage of total steps for cooldown/decay
cooldown_pct = 0.10 # e.g., last 10% of steps
min_lr_ratio = 0.1

[hyperparameters.optimizer]
name = "adamw"      # adamw | muon
weight_decay = 0.01
beta1 = 0.9
beta2 = 0.95
eps = 1e-8

[batch]
batch_size = 1024
# micro_batch_size = 256         # optional per-device micro batch; enables grad accumulation

[batch.adaptive]
enabled = false
double_at_lr_ratio = 0.5
quadruple_at_lr_ratio = 0.25

[dropout]
dropout_prob = 0.1
attention_dropout_prob = 0.0

[binning]
# strategy = "edges" | "upper_bounds"
strategy = "edges"
# ensure exact 0 and exact 1 are isolated into their own bins
special_zero_one = true
# Semantic edges for EV discretization (default used if omitted)
edges = [0.0, 0.5, 0.8, 0.9, 0.95, 0.98, 0.99, 0.995, 0.998, 0.999, 1.0]

[dataset]
# Root directory containing steps.npy and metadata.db (resolved relative to repo root)
dataset_dir = "./dataset_dir"

# Universe selection via SQL (optional). When omitted, uses all runs.
# Example filter: only runs with score between [50k, 500k]
run_sql = "SELECT id FROM runs WHERE max_score BETWEEN ? AND ?"
sql_params = [50_000, 500_000]

# For quick validation, run a fixed number of steps
num_steps = 100_000
# Or drive by epochs instead (steps takes priority if both set)
# num_epochs = 4

# Validation: choose disjoint runs for val
# Option A: random run split (percentage of unique runs)
val_run_pct = 0.10
val_split_seed = 42
# Option B: explicit validation SQL (takes precedence if set)
# val_run_sql = "SELECT id FROM runs WHERE ..."
# val_sql_params = []

# Validate every N training steps (0 disables)
val_every = 1000

[dataset.rotation_augment]
mode = "none"         # none | random_k
# seed = 42             # optional: fixed seed for deterministic rotation
# allow_noop = true     # if false, k=0 (no rotation) is excluded from random_k

[dataset.flip_augment]
mode = "none"         # none | random_axis
# seed = 42             # optional: fixed seed for deterministic flip
# allow_noop = true     # if false, axis=0 (no flip) is excluded from random_axis

# Shard locality controls (optional). Enable to keep one shard in RAM and shuffle locally.
# shard_locality = true
# shard_locality_block_size = 500000    # Draw at most this many rows per shard before rotating
# shard_cache_in_memory = true          # Materialise the active shard into RAM
# shard_cache_keep_shards = 1           # Number of cached shards to retain simultaneously

[checkpoint]
# every_epochs = 1
# save_best_every_steps = 1000
# best_min_delta = 0.0
# save_pt_every_steps = 5000    # Persist model-step-XXXXXXXX.pt bundles for resume

[amp]
# Keep fp32 master weights while running bf16 autocast on CUDA.
# Set false to store weights in bf16 instead.
master_weights_fp32 = true
