init_dir = "inits/v1_10m"      # Directory with config & weights or a checkpoint .pt file
checkpoint_dir = "checkpoints"
seed = 0

[wandb]
enabled = true
project = "train-2048"
entity = ""
run_name = ""
tags = []
mode = "online"        # online | offline | disabled
report_every = 10       # log to W&B every N steps (>=1)

[hyperparameters]
learning_rate = 4e-4
muon_lr = 2e-2
# Clip global gradient norm each step (None to disable)
grad_clip_norm = 1.0

[hyperparameters.lr_schedule]
name = "warmup-stable-decay" # constant | warmup-stable-decay | cosine
warmup_steps = 1000
# decay_steps = 0
# Alternatively, specify a percentage of total steps for cooldown/decay
cooldown_pct = 0.10 # e.g., last 10% of steps
min_lr_ratio = 0.1

[hyperparameters.optimizer]
name = "adamw"      # adamw | muon
weight_decay = 0.01
beta1 = 0.9
beta2 = 0.95
eps = 1e-8

[batch]
batch_size = 1024
# micro_batch_size = 256         # optional per-device micro batch; enables grad accumulation

[batch.adaptive]
enabled = false
double_at_lr_ratio = 0.5
quadruple_at_lr_ratio = 0.25

[dropout]
dropout_prob = 0.1
attention_dropout_prob = 0.0

[binning]
# strategy = "edges" | "upper_bounds"
strategy = "edges"
# ensure exact 0 and exact 1 are isolated into their own bins
special_zero_one = true
# Semantic edges for EV discretization (default used if omitted)
edges = [0.0, 0.5, 0.8, 0.9, 0.95, 0.98, 0.99, 0.995, 0.998, 0.999, 1.0]

# Optional: supervised value head training (e.g., return prediction probe)
[value_training]
# Enable to consume values.npy/values-*.npy sidecar and train the value head.
enabled = false
# Loss type for the value head.
objective = "mse"           # mse | cross_entropy
# Target source from the value sidecar (return_scaled is default and recommended).
target = "return_scaled"    # return_scaled | return_raw
# Loss weights (set policy_loss_weight=0 or value_only=true for value-only training).
loss_weight = 1.0
policy_loss_weight = 1.0
# Extra scale for the value loss relative to the policy component.
value_loss_policy_scale = 1.0
value_only = false
# Freeze encoder/trunk when training a value probe on a fixed policy.
freeze_trunk = false
# Cross-entropy/two-hot support (MuZero transform). Range must match vocab_size-1.
# ce_vocab_size = 601
# ce_support_min = 0.0
# ce_support_max = 600.0
# ce_transform_epsilon = 0.001
# ce_apply_transform = false   # set true when using return_raw; keep false for return_scaled

[dataset]
# Root directory containing steps.npy and metadata.db (resolved relative to repo root)
dataset_dir = "./dataset_dir"

# Universe selection via SQL (optional). When omitted, uses all runs.
# Example filter: only runs with score between [50k, 500k]
run_sql = "SELECT id FROM runs WHERE max_score BETWEEN ? AND ?"
sql_params = [50_000, 500_000]

# For quick validation, run a fixed number of steps
num_steps = 100_000
# Or drive by epochs instead (steps takes priority if both set)
# num_epochs = 4

# Validation: choose disjoint runs for val
# Option A: random run split (percentage of unique runs)
val_run_pct = 0.10
val_split_seed = 42
# Option B: explicit validation SQL (takes precedence if set)
# val_run_sql = "SELECT id FROM runs WHERE ..."
# val_sql_params = []

# Validate every N training steps (0 disables)
val_every = 1000

# Shard locality controls (optional). Enable to keep one shard in RAM and shuffle locally.
# shard_locality = true
# shard_locality_block_size = 500000    # Draw at most this many rows per shard before rotating
# shard_cache_in_memory = true          # Materialise the active shard into RAM
# shard_cache_keep_shards = 1           # Number of cached shards to retain simultaneously

[checkpoint]
# every_epochs = 1
# save_best_every_steps = 1000
# best_min_delta = 0.0
# save_pt_every_steps = 5000    # Persist model-step-XXXXXXXX.pt bundles for resume
