init_dir = "inits/v1_10m"
checkpoint_dir = "checkpoints"
seed = 0

[wandb]
enabled = true
project = "train-2048"
entity = ""
run_name = ""
tags = []
mode = "online"        # online | offline | disabled
report_every = 10       # log to W&B every N steps (>=1)

[hyperparameters]
learning_rate = 4e-4
muon_lr = 2e-2

[hyperparameters.lr_schedule]
name = "warmup-stable-decay" # constant | warmup-stable-decay
warmup_steps = 1000
# decay_steps = 0
# Alternatively, specify a percentage of total steps for cooldown/decay
cooldown_pct = 0.10 # e.g., last 10% of steps
min_lr_ratio = 0.1

[hyperparameters.optimizer]
name = "adamw"      # adamw | muon
weight_decay = 0.01
beta1 = 0.9
beta2 = 0.95
eps = 1e-8

[batch]
batch_size = 1024
# micro_batch_size = 256

[dropout]
dropout_prob = 0.1
attention_dropout_prob = 0.0

[binning]
# strategy = "edges" | "upper_bounds"
strategy = "edges"
# ensure exact 0 and exact 1 are isolated into their own bins
special_zero_one = true
# Semantic edges for EV discretization (default used if omitted)
edges = [0.0, 0.5, 0.8, 0.9, 0.95, 0.98, 0.99, 0.995, 0.998, 0.999, 1.0]

[dataset]
# Root directory containing steps.npy and metadata.db (resolved relative to repo root)
dataset_dir = "./dataset_dir"

# Universe selection via SQL (optional). When omitted, uses all runs.
# Example filter: only runs with score between [50k, 500k]
run_sql = "SELECT id FROM runs WHERE max_score BETWEEN ? AND ?"
sql_params = [50_000, 500_000]

# For quick validation, run a fixed number of steps
num_steps = 100_000
# Or drive by epochs instead (steps takes priority if both set)
# num_epochs = 4

# Validation: choose disjoint runs for val
# Option A: random run split (percentage of unique runs)
val_run_pct = 0.10
val_split_seed = 42
# Option B: explicit validation SQL (takes precedence if set)
# val_run_sql = "SELECT id FROM runs WHERE ..."
# val_sql_params = []

# Validate every N training steps (0 disables)
val_every = 1000
